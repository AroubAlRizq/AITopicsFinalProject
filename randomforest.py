# -*- coding: utf-8 -*-
"""RandomForest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aDDQKwQ1M0si44XoZ82bwESnTTTVhFDQ

# **Imports**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import zscore
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    classification_report,
    confusion_matrix,
    RocCurveDisplay
)

"""# **Load The Dataset**"""

#Loading Data
data = pd.read_csv("/content/Dry_Bean_Dataset.csv")

# Ensure the data is loaded correctly
data.info()

"""# **Preprocessing The Dataset**

**Handling Missing Values**
"""

# Handle missing values if any (check for NaNs)
print('Number of Null values:', data.isnull().sum().sum())

"""**Remove Outliers**"""

# Identify and remove outliers using Z-score
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns
z_scores = zscore(data[numerical_columns])
outliers = (np.abs(z_scores) > 3).any(axis=1)
data = data[~outliers].reset_index(drop=True)
print("Final shape after outlier removal:", data.shape)

"""**Separate Target**"""

# Separate features and target
X = data.drop(columns=['Class'])
y = data['Class']

# Print to check
print(X.head())
print(y.head())

"""**Split The Dataset Into Training and Testing**"""

# Split the data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the data to ensure optimal RF performance
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Print to check
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

"""# **Random Forest Implementation**"""

# Define Random Forest
rf = RandomForestClassifier(random_state=42)

# Set Hyperparameters
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Use GridSearchCV for finding best hyperparameters
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Print the best hyperparameters
print("Best Hyperparameters:", grid_search.best_params_)

"""# **Training The Model**"""

# Train the model with the best hyperparameters
best_rf = grid_search.best_estimator_
best_rf.fit(X_train, y_train)

# Make predictions
y_pred = best_rf.predict(X_test)

"""# **Evaluation**"""

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

# Print the evaluation metrics
print("Evaluation Metrics:")
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

# Print Classifcation Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Create Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))

# Plot The Confusion Matrix
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=best_rf.classes_, yticklabels=best_rf.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()