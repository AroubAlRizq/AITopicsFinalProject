# -*- coding: utf-8 -*-
"""Project_Knn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mk6buEmpZlNXjeoJRKK52XFxIRwwhz0W
"""

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')
df = pd.read_csv('/content/drive/MyDrive/Dry_Bean_Dataset.csv')

df.head()

# Handle missing values if any (check for NaNs)
print('Number of Null values:', df.isnull().sum().sum())

from scipy.stats import zscore

# Identify and remove outliers using Z-score
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
z_scores = zscore(df[numerical_columns])
outliers = (np.abs(z_scores) > 3).any(axis=1)
df = df[~outliers].reset_index(drop=True)
print("Final shape after outlier removal:", df.shape)

# Separate features and target
X = df.drop(columns=['Class'])
y = df['Class']

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Split the data into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the data to ensure optimal KNN performance
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.neighbors import KNeighborsClassifier

# Define the KNN model
knn = KNeighborsClassifier()

# Set up the hyperparameter grid for 'k'
param_grid = {'n_neighbors': range(1, 21)}

# Use GridSearchCV for hyperparameter tuning with cross-validation
grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Print the best hyperparameters
print("Best hyperparameters found:", grid_search.best_params_)

# Train the KNN model with the optimal parameters
best_knn = grid_search.best_estimator_

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

# Make predictions on the test set
y_pred = best_knn.predict(X_test)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

# Print metrics
print("Model Accuracy:", accuracy)
print("Model Precision:", precision)
print("Model Recall:", recall)
print("Model F1-Score:", f1)

# Print a detailed classification report
print("Classification Report:\n", classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Compute the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=best_knn.classes_, yticklabels=best_knn.classes_)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()